{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Apache Spark : Analyse des données climatiques mondiales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectif :\n",
    "Analyser les tendances climatiques mondiales à l'aide de Spark, y compris le nettoyage des données, l'EDA et l'extraction d'informations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groupe :\n",
    "#### Bérenger AKODO \n",
    "#### Etienne VACHER\n",
    "#### Mamoudou NDONGO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jeu de données :(Sur 50 ans :: 1929 - 1977)\n",
    "[Global Surface Summary of the Day (GSOD) provenant de NOAA](https://www.ncei.noaa.gov/access/metadata/landing-page/bin/iso?id=gov.noaa.ncdc:C00516)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('STATION', 'bigint'), ('DATE', 'date'), ('LATITUDE', 'double'), ('LONGITUDE', 'double'), ('ELEVATION', 'double'), ('NAME', 'string'), ('TEMP', 'double'), ('TEMP_ATTRIBUTES', 'double'), ('DEWP', 'double'), ('DEWP_ATTRIBUTES', 'double'), ('SLP', 'double'), ('SLP_ATTRIBUTES', 'double'), ('STP', 'double'), ('STP_ATTRIBUTES', 'double'), ('VISIB', 'double'), ('VISIB_ATTRIBUTES', 'double'), ('WDSP', 'double'), ('WDSP_ATTRIBUTES', 'double'), ('MXSPD', 'double'), ('GUST', 'double'), ('MAX', 'double'), ('MAX_ATTRIBUTES', 'string'), ('MIN', 'double'), ('MIN_ATTRIBUTES', 'string'), ('PRCP', 'double'), ('PRCP_ATTRIBUTES', 'string'), ('SNDP', 'double'), ('FRSHTT', 'int')]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Créer ou obtenir une session Spark active\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Meteo Analysis\") \\\n",
    "    .config(\"spark.network.timeout\", \"14400s\") \\\n",
    "    .config(\"spark.executor.memory\", \"10g\") \\\n",
    "    .config(\"spark.driver.memory\", \"10g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Définir le chemin vers les fichiers CSV contenant les données des cyclistes\n",
    "path = \"./datas/50/*/*.csv\"\n",
    "\n",
    "# Charger les données CSV dans un DataFrame Spark\n",
    "climat = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(path)\n",
    "\n",
    "# Afficher les types de données (schéma) de chaque colonne dans le DataFrame\n",
    "print(climat.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: long (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4013704, 28)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspection du schéma et calcul des statistiques\n",
    "climat.printSchema()\n",
    "climat.count(), len(climat.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4013704"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- STATION: long (nullable = true)\n",
      " |-- DATE: date (nullable = true)\n",
      " |-- LATITUDE: double (nullable = true)\n",
      " |-- LONGITUDE: double (nullable = true)\n",
      " |-- ELEVATION: double (nullable = true)\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- TEMP: double (nullable = true)\n",
      " |-- TEMP_ATTRIBUTES: double (nullable = true)\n",
      " |-- DEWP: double (nullable = true)\n",
      " |-- DEWP_ATTRIBUTES: double (nullable = true)\n",
      " |-- SLP: double (nullable = true)\n",
      " |-- SLP_ATTRIBUTES: double (nullable = true)\n",
      " |-- STP: double (nullable = true)\n",
      " |-- STP_ATTRIBUTES: double (nullable = true)\n",
      " |-- VISIB: double (nullable = true)\n",
      " |-- VISIB_ATTRIBUTES: double (nullable = true)\n",
      " |-- WDSP: double (nullable = true)\n",
      " |-- WDSP_ATTRIBUTES: double (nullable = true)\n",
      " |-- MXSPD: double (nullable = true)\n",
      " |-- GUST: double (nullable = true)\n",
      " |-- MAX: double (nullable = true)\n",
      " |-- MAX_ATTRIBUTES: string (nullable = true)\n",
      " |-- MIN: double (nullable = true)\n",
      " |-- MIN_ATTRIBUTES: string (nullable = true)\n",
      " |-- PRCP: double (nullable = true)\n",
      " |-- PRCP_ATTRIBUTES: string (nullable = true)\n",
      " |-- SNDP: double (nullable = true)\n",
      " |-- FRSHTT: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "climat.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3438"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "climat.select(\"STATION\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifier les colonnes avec des valeurs manquantes et les traiter\n",
    "missing_values = climat.select([F.count(F.when(F.col(c).isNull(), c)) for c in climat.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supprimer les lignes avec des valeurs critiques manquantes\n",
    "climat_annee_delete = climat.dropna(subset=['TEMP', 'DATE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'STATION': 0, 'DATE': 0, 'LATITUDE': 107936, 'LONGITUDE': 107936, 'ELEVATION': 108599, 'NAME': 106035, 'TEMP': 0, 'TEMP_ATTRIBUTES': 0, 'DEWP': 0, 'DEWP_ATTRIBUTES': 0, 'SLP': 0, 'SLP_ATTRIBUTES': 0, 'STP': 0, 'STP_ATTRIBUTES': 0, 'VISIB': 0, 'VISIB_ATTRIBUTES': 0, 'WDSP': 0, 'WDSP_ATTRIBUTES': 0, 'MXSPD': 0, 'GUST': 0, 'MAX': 0, 'MAX_ATTRIBUTES': 0, 'MIN': 0, 'MIN_ATTRIBUTES': 0, 'PRCP': 0, 'PRCP_ATTRIBUTES': 0, 'SNDP': 0, 'FRSHTT': 0}\n"
     ]
    }
   ],
   "source": [
    "# Calculer le nombre de valeurs manquantes pour chaque colonne\n",
    "missing_values = climat.select(\n",
    "    [count(when(col(c).isNull(), c)).alias(c) for c in climat.columns]\n",
    ")\n",
    "\n",
    "# Afficher chaque ligne (ici une seule ligne avec les résultats pour chaque colonne)\n",
    "for row in missing_values.collect():\n",
    "    print(row.asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'enregistrements après nettoyage : 3904665\n"
     ]
    }
   ],
   "source": [
    "# Supprimer les lignes avec des valeurs manquantes dans n'importe quelle colonne\n",
    "drop_climat = climat.dropna()\n",
    "\n",
    "# Compter le nombre d'enregistrements restants après nettoyage\n",
    "remaining_records = drop_climat.count()\n",
    "\n",
    "print(f\"Nombre d'enregistrements après nettoyage : {remaining_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre d'enregistrements après nettoyage : 4013704\n"
     ]
    }
   ],
   "source": [
    "#Pour l'ensemble de notre étude, nous avons choisi d'adopter la méthode d'imputation des \n",
    "#valeurs manquantes en utilisant une valeur fixe, telle que la moyenne, la médiane, ou d'autres \n",
    "#valeurs appropriées.\n",
    "\n",
    "# Récupérer la liste des colonnes numériques\n",
    "numeric_columns = [field.name for field in climat.schema.fields if isinstance(field.dataType, (IntegerType, DoubleType))]\n",
    "\n",
    "# Calculer la moyenne de chaque colonne numérique et remplir les valeurs manquantes\n",
    "mean_values = climat.select([F.mean(c).alias(c) for c in numeric_columns]).collect()[0].asDict()\n",
    "\n",
    "# Remplacer les valeurs manquantes pour chaque colonne par la moyenne\n",
    "climat_cleaned = climat.fillna(mean_values)\n",
    "\n",
    "# Afficher le nombre d'enregistrements après nettoyage\n",
    "remaining_records = climat_cleaned.count()\n",
    "print(f\"Nombre d'enregistrements après nettoyage : {remaining_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+\n",
      "|TEMP|TEMP_C|\n",
      "+----+------+\n",
      "|47.2|   8.4|\n",
      "|33.0|   0.6|\n",
      "|26.7|  -2.9|\n",
      "|26.2|  -3.2|\n",
      "|31.4|  -0.3|\n",
      "|27.3|  -2.6|\n",
      "|20.2|  -6.6|\n",
      "|18.6|  -7.4|\n",
      "|25.6|  -3.6|\n",
      "|30.5|  -0.8|\n",
      "|21.1|  -6.1|\n",
      "|31.2|  -0.4|\n",
      "|33.1|   0.6|\n",
      "|36.0|   2.2|\n",
      "|44.8|   7.1|\n",
      "|33.8|   1.0|\n",
      "|35.5|   1.9|\n",
      "|38.1|   3.4|\n",
      "|29.5|  -1.4|\n",
      "|36.7|   2.6|\n",
      "+----+------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Colonnes restantes : ['STATION', 'DATE', 'LATITUDE', 'LONGITUDE', 'ELEVATION', 'NAME', 'TEMP', 'DEWP', 'SLP', 'STP', 'VISIB', 'WDSP', 'MXSPD', 'GUST', 'MAX', 'MIN', 'PRCP', 'SNDP', 'FRSHTT', 'TEMP_C']\n"
     ]
    }
   ],
   "source": [
    "def remove_unwanted_columns(climat_cleaned: DataFrame, columns_to_remove: list) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Supprime les colonnes spécifiées d'un DataFrame PySpark.\n",
    "    \n",
    "    :param df: DataFrame d'entrée\n",
    "    :param columns_to_remove: Liste des colonnes à supprimer\n",
    "    :return: DataFrame nettoyé sans les colonnes spécifiées\n",
    "    \"\"\"\n",
    "    return climat_cleaned.drop(*columns_to_remove)\n",
    "\n",
    "# Liste des colonnes à supprimer\n",
    "columns_to_remove = [\n",
    "    \"TEMP_ATTRIBUTES\",\n",
    "    \"DEWP_ATTRIBUTES\",\n",
    "    \"SLP_ATTRIBUTES\",\n",
    "    \"STP_ATTRIBUTES\",\n",
    "    \"VISIB_ATTRIBUTES\",\n",
    "    \"WDSP_ATTRIBUTES\",\n",
    "    \"MAX_ATTRIBUTES\",\n",
    "    \"MIN_ATTRIBUTES\",\n",
    "    \"PRCP_ATTRIBUTES\"\n",
    "]\n",
    "\n",
    "# Suppression des colonnes dans le DataFrame climat\n",
    "climat_col_cleaned = remove_unwanted_columns(climat_cleaned, columns_to_remove)\n",
    "#Rajout de la colonne Celcius\n",
    "climat_col_cleaned = climat_col_cleaned.withColumn(\"TEMP_C\", bround((col(\"TEMP\") - 32) / 1.8, 1))\n",
    " \n",
    "# Afficher le DataFrame avec les colonnes 'TEMP' et 'TEMP_C' arrondies\n",
    "climat_col_cleaned.select(\"TEMP\", \"TEMP_C\").show()\n",
    "# Afficher les colonnes restantes pour vérifier\n",
    "print(\"Colonnes restantes :\", climat_col_cleaned.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+---------+---------+--------------------+----+----+------+-----+-----+----+-----+-----+----+----+-----+-----+------+------+----+-----+---+------+\n",
      "|    STATION|      DATE|LATITUDE|LONGITUDE|ELEVATION|                NAME|TEMP|DEWP|   SLP|  STP|VISIB|WDSP|MXSPD| GUST| MAX| MIN| PRCP| SNDP|FRSHTT|TEMP_C|year|month|day|is_hot|\n",
      "+-----------+----------+--------+---------+---------+--------------------+----+----+------+-----+-----+----+-----+-----+----+----+-----+-----+------+------+----+-----+---+------+\n",
      "|99999914768|1952-01-01|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|47.2|43.1|1011.9|991.5|  9.3|13.8| 19.0|999.9|53.1|39.9|99.99|999.9|110000|   8.4|1952|    1|  1| false|\n",
      "|99999914768|1952-01-02|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|33.0|27.9|1025.7|  4.7| 11.9| 9.0| 15.0|999.9|39.0|30.0|  0.0|999.9|     0|   0.6|1952|    1|  2| false|\n",
      "|99999914768|1952-01-03|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|26.7|21.4|1024.5|  3.3|  7.1|11.2| 22.9|999.9|30.0|19.9|99.99|999.9|  1000|  -2.9|1952|    1|  3| false|\n",
      "|99999914768|1952-01-04|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|26.2|21.1|1027.2|  6.0|  8.2| 7.6| 14.0|999.9|36.0|17.1|99.99|999.9|  1000|  -3.2|1952|    1|  4| false|\n",
      "|99999914768|1952-01-05|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|31.4|25.9|1012.0|991.1|  7.9|13.9| 20.0|999.9|35.1|28.0|99.99|999.9|  1000|  -0.3|1952|    1|  5| false|\n",
      "|99999914768|1952-01-06|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|27.3|21.9|1019.8|998.7|  8.3| 7.5| 12.0|999.9|30.0|21.9|99.99|999.9|  1000|  -2.6|1952|    1|  6| false|\n",
      "|99999914768|1952-01-07|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|20.2|14.4|1026.6|  5.3| 11.4| 5.6| 10.1|999.9|27.0|12.9|99.99|999.9|  1000|  -6.6|1952|    1|  7| false|\n",
      "|99999914768|1952-01-08|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|18.6|14.1|1020.7|999.3|  8.7| 4.9|  8.9|999.9|26.1|12.0|99.99|999.9|  1000|  -7.4|1952|    1|  8| false|\n",
      "|99999914768|1952-01-09|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|25.6|22.3|1010.4|989.4|  3.9| 4.6|  8.9|999.9|30.9|21.0|99.99|999.9|111000|  -3.6|1952|    1|  9| false|\n",
      "|99999914768|1952-01-10|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|30.5|27.5|1012.7|991.7|  5.1| 9.3| 13.0|999.9|34.0|23.0|99.99|999.9|111000|  -0.8|1952|    1| 10| false|\n",
      "|99999914768|1952-01-11|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|21.1|15.0|1030.0|  8.5| 13.7|10.1| 15.9|999.9|28.9|14.0|99.99|999.9|  1000|  -6.1|1952|    1| 11| false|\n",
      "|99999914768|1952-01-12|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|31.2|25.7|1023.6|  2.5|  8.2|10.5| 17.1|999.9|37.9|19.9|99.99|999.9|  1000|  -0.4|1952|    1| 12| false|\n",
      "|99999914768|1952-01-13|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|33.1|27.9|1022.4|  1.4|  6.3| 5.2|  8.9|999.9|42.1|28.0|  0.0|999.9|100000|   0.6|1952|    1| 13| false|\n",
      "|99999914768|1952-01-14|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|36.0|33.9|1015.6|994.7|  3.4| 4.7|  8.9|999.9|39.0|33.1|99.99|999.9|110000|   2.2|1952|    1| 14| false|\n",
      "|99999914768|1952-01-15|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|44.8|39.2|1009.9|989.3|  8.7|17.6| 39.0|999.9|60.1|33.1|99.99|999.9|110000|   7.1|1952|    1| 15| false|\n",
      "|99999914768|1952-01-16|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|33.8|26.1|1023.3|  2.4| 12.9|16.3| 24.1|999.9|39.0|28.0|  0.0|999.9|     0|   1.0|1952|    1| 16| false|\n",
      "|99999914768|1952-01-17|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|35.5|30.8|1016.4|995.5|  9.0| 9.8| 22.0|999.9|48.9|25.0|  0.0|999.9|     0|   1.9|1952|    1| 17| false|\n",
      "|99999914768|1952-01-18|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|38.1|30.2|1005.2|984.7| 11.0|20.5| 31.1|999.9|52.0|30.0|99.99|999.9| 10010|   3.4|1952|    1| 18| false|\n",
      "|99999914768|1952-01-19|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|29.5|20.6|1019.3|998.1| 10.3| 8.7| 22.0|999.9|37.0|23.0|99.99|999.9|  1000|  -1.4|1952|    1| 19| false|\n",
      "|99999914768|1952-01-20|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|36.7|32.3|1000.5|979.9|  8.9|16.5| 28.0|999.9|43.0|30.0|99.99|999.9| 11000|   2.6|1952|    1| 20| false|\n",
      "+-----------+----------+--------+---------+---------+--------------------+----+----+------+-----+-----+----+-----+-----+----+----+-----+-----+------+------+----+-----+---+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transformer le jeu de données\n",
    "# Ajouter de nouvelles colonnes pour l'année, le mois et le jour\n",
    "climat_transformed = (\n",
    "    climat_col_cleaned\n",
    "    .withColumn('year', F.year(F.col('DATE')))  # Extraire l'année\n",
    "    .withColumn('month', F.month(F.col('DATE')))  # Extraire le mois\n",
    "    .withColumn('day', F.dayofmonth(F.col('DATE')))  # Extraire le jour\n",
    ")\n",
    "\n",
    "# Classifier les jours chauds (les données sont en degrées F°)\n",
    "climat_transformed = climat_transformed.withColumn('is_hot', F.col('TEMP') > 68)\n",
    "\n",
    "climat_transformed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------+---------+---------+--------------------+----+----+------+-----+-----+----+-----+-----+----+----+-----+-----+------+------+----+-----+---+------+\n",
      "|    STATION|      DATE|LATITUDE|LONGITUDE|ELEVATION|                NAME|TEMP|DEWP|   SLP|  STP|VISIB|WDSP|MXSPD| GUST| MAX| MIN| PRCP| SNDP|FRSHTT|TEMP_C|year|month|day|is_hot|\n",
      "+-----------+----------+--------+---------+---------+--------------------+----+----+------+-----+-----+----+-----+-----+----+----+-----+-----+------+------+----+-----+---+------+\n",
      "|99999914768|1952-01-01|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|47.2|43.1|1011.9|991.5|  9.3|13.8| 19.0|999.9|53.1|39.9|99.99|999.9|110000|   8.4|1952|    1|  1| false|\n",
      "|99999914768|1952-01-02|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|33.0|27.9|1025.7|  4.7| 11.9| 9.0| 15.0|999.9|39.0|30.0|  0.0|999.9|     0|   0.6|1952|    1|  2| false|\n",
      "|99999914768|1952-01-03|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|26.7|21.4|1024.5|  3.3|  7.1|11.2| 22.9|999.9|30.0|19.9|99.99|999.9|  1000|  -2.9|1952|    1|  3| false|\n",
      "|99999914768|1952-01-04|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|26.2|21.1|1027.2|  6.0|  8.2| 7.6| 14.0|999.9|36.0|17.1|99.99|999.9|  1000|  -3.2|1952|    1|  4| false|\n",
      "|99999914768|1952-01-05|43.11723|-77.67539|    164.5|FREDERICK DOUGLAS...|31.4|25.9|1012.0|991.1|  7.9|13.9| 20.0|999.9|35.1|28.0|99.99|999.9|  1000|  -0.3|1952|    1|  5| false|\n",
      "+-----------+----------+--------+---------+---------+--------------------+----+----+------+-----+-----+----+-----+-----+----+----+-----+-----+------+------+----+-----+---+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "climat_transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total de jours classés comme 'chauds' : 1110333\n"
     ]
    }
   ],
   "source": [
    "# Compter le nombre de jours chauds (is_hot = true)\n",
    "hot_days_count = climat_transformed.filter(F.col(\"is_hot\") == True).count()\n",
    "\n",
    "# Afficher le résultat\n",
    "print(f\"Nombre total de jours classés comme 'chauds' : {hot_days_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous avons supprimé les colonnes qui nous semblaient inutiles, géré les valeurs manquantes, \n",
    "#ajouté de nouvelles colonnes, et créé une colonne supplémentaire intitulée is_hot et une autre colonne pour\n",
    "# transformer les degrées Fahrenheit en degrées Celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer les moyennes annuelles et autres statistiques\n",
    "# avg_temp_by_year = df_transformed.groupBy('year').avg('temperature')\n",
    "#avg_temp_by_year.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+------------------------+\n",
      "|Mean_Temperature_Fahrenheit|Mean_Temperature_Celcius|\n",
      "+---------------------------+------------------------+\n",
      "|                       NULL|                    NULL|\n",
      "+---------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Définir le chemin vers les fichiers CSV contenant les données des cyclistes\n",
    "climat_2020 = climat_transformed.filter(F.col('year') == 2020)\n",
    "\n",
    "avg_temp_2020 = climat_2020.agg({\"TEMP\": \"mean\", \"TEMP_C\": \"mean\"}).withColumnRenamed(\"avg(TEMP)\", \"Mean_Temperature_Fahrenheit\") \\\n",
    "                                                                   .withColumnRenamed(\"avg(TEMP_C)\", \"Mean_Temperature_Celcius\")\n",
    "\n",
    "# Afficher les résultats\n",
    "avg_temp_2020.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+----------+---------------------------+------------------------+\n",
      "|    STATION|                NAME|  LATITUDE| LONGITUDE|Mean_Temperature_Fahrenheit|Mean_Temperature_Celcius|\n",
      "+-----------+--------------------+----------+----------+---------------------------+------------------------+\n",
      "|74901099999|LAGUNA ARMY AIR F...|     32.85|    -114.4|          94.21052631578948|       34.56578947368422|\n",
      "|74926499999|         KYI EIK, BM|    21.633|    96.033|          90.30555555555556|      32.388888888888886|\n",
      "|48053099999|        MEIKTILA, BM|20.8333333|95.8333333|          89.62926829268297|      32.012195121951216|\n",
      "|48047099999|        MYINGYAN, BM|21.4666666|95.3833333|          88.69666666666666|      31.500000000000007|\n",
      "|42705099999|         PURULIA, IN|23.3333333|86.4166666|          87.34387755102044|       30.74897959183672|\n",
      "+-----------+--------------------+----------+----------+---------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculer la température moyenne par station et obtenir les autres informations\n",
    "avg_temp_by_station = climat_transformed.groupBy('STATION', 'NAME', 'LATITUDE', 'LONGITUDE').agg({\"TEMP\": \"mean\", \"TEMP_C\": \"mean\"}).withColumnRenamed(\"avg(TEMP)\", \"Mean_Temperature_Fahrenheit\") \\\n",
    "                                                                                                                                    .withColumnRenamed(\"avg(TEMP_C)\", \"Mean_Temperature_Celcius\")\n",
    "# Trier par température moyenne (ordre décroissant) et prendre les 5 premières stations\n",
    "top_5_stations = avg_temp_by_station.orderBy(col('Mean_Temperature_Fahrenheit').desc()).limit(5)\n",
    " \n",
    "# Afficher les résultats\n",
    "top_5_stations.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------------------+\n",
      "|year|total_precipitation|\n",
      "+----+-------------------+\n",
      "|1929|  94335.60999999994|\n",
      "|1930| 283451.13999999996|\n",
      "|1931| 268856.46000000037|\n",
      "|1932|  266578.7199999999|\n",
      "|1933| 396390.39999999997|\n",
      "|1934| 464106.10999999975|\n",
      "|1935| 495380.50999999995|\n",
      "|1936|  949142.6699999995|\n",
      "|1937| 2001261.3600000031|\n",
      "|1938|  873775.1600000003|\n",
      "|1939|          927931.89|\n",
      "|1940|  1189441.350000002|\n",
      "|1941| 1685390.5900000026|\n",
      "|1942|  3192311.150000006|\n",
      "|1943| 6350615.9100000225|\n",
      "|1944|  9241402.879999947|\n",
      "|1945|  9786895.800000032|\n",
      "|1946|   4536058.27000001|\n",
      "|1947|  4735983.610000031|\n",
      "|1948|  9710215.549999936|\n",
      "+----+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Calculer la somme des précipitations mondiales par année\n",
    "# Calculer la température moyenne par station et obtenir les autres informations\n",
    "total_precipitation = climat_transformed.groupBy('year').agg({\"PRCP\": \"sum\"}).withColumnRenamed(\"sum(PRCP)\", \"total_precipitation\")\n",
    "                                                                                                                                \n",
    "# Trier par température moyenne (ordre décroissant) et prendre les 5 premières stations\n",
    "precip_by_year = total_precipitation.orderBy(col('year').desc()).limit(50)\n",
    "\n",
    "# Afficher l'évolution des précipitations par année\n",
    "precip_by_year.orderBy('year').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enregistrer comme vue SQL temporaire et exécuter des requêtes\n",
    "climat_transformed.createOrReplaceTempView('climate')\n",
    "# spark.sql('SELECT ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+------------------------+\n",
      "|year|Mean_Temperature_Fahrenheit|Mean_Temperature_Celcius|\n",
      "+----+---------------------------+------------------------+\n",
      "|1937|          44.56013802362421|       6.978064607303943|\n",
      "+----+---------------------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculer la température moyenne par année\n",
    "coldest_month = spark.sql('''\n",
    "    SELECT year, AVG(TEMP) AS Mean_Temperature_Fahrenheit, AVG(TEMP_C) AS Mean_Temperature_Celcius\n",
    "    FROM climate\n",
    "    GROUP BY year\n",
    "    ORDER BY Mean_Temperature_Fahrenheit ASC\n",
    "    LIMIT 1\n",
    "''')\n",
    "\n",
    "# Afficher le mois le plus froid et sa température moyenne\n",
    "coldest_month.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+---------------------+\n",
      "|STATION    |NAME            |Nombre_Enregistrement|\n",
      "+-----------+----------------+---------------------+\n",
      "|72286023119|MARCH AFB, CA US|7271                 |\n",
      "+-----------+----------------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Trouver la station avec le plus grand nombre d'enregistrements\n",
    "most_records_station = spark.sql('''\n",
    "    SELECT STATION, NAME, COUNT(*) AS Nombre_Enregistrement\n",
    "    FROM climate\n",
    "    GROUP BY STATION, NAME\n",
    "    ORDER BY Nombre_Enregistrement DESC\n",
    "    LIMIT 1\n",
    "''')\n",
    "\n",
    "# Afficher le résultat\n",
    "most_records_station.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour trouver le mois le plus froid et la température moyenne :\n",
    "    #SELECT year, AVG(TEMP) AS Mean_Temperature_Fahrenheit, AVG(TEMP_C) AS Mean_Temperature_Celcius\n",
    "    #FROM climate\n",
    "    #GROUP BY year\n",
    "    #ORDER BY Mean_Temperature_Fahrenheit ASC\n",
    "    #LIMIT 1\n",
    "#Pour trouver la station avec le plus grand nombre d'enregistrements :\n",
    "    #SELECT STATION, NAME, COUNT(*) AS Nombre_Enregistrement\n",
    "    #FROM climate\n",
    "    #GROUP BY STATION, NAME\n",
    "    #ORDER BY Nombre_Enregistrement DESC\n",
    "    #LIMIT 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporter les données vers Pandas et créer des visualisations\n",
    "# import matplotlib.pyplot as plt\n",
    "# df_pandas = avg_temp_by_year.toPandas()\n",
    "# plt.plot(df_pandas['year'], df_pandas['avg_temperature'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Undefined error message parameter for error class: CANNOT_PARSE_DATATYPE. Parameters: {'error': 'An error occurred while calling o409.schema'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:572\u001b[0m, in \u001b[0;36mDataFrame.schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    570\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m cast(\n\u001b[0;32m--> 572\u001b[0m         StructType, _parse_datatype_json_string(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    573\u001b[0m     )\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:334\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 334\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    335\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name))\n\u001b[1;32m    337\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mPy4JError\u001b[0m: An error occurred while calling o409.schema",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convertir le DataFrame PySpark en DataFrame Pandas\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#climat_transformed_small = climat_transformed.limit(20)  # Exemple : limiter à 1000 lignes\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#climat_transformed_pandas = climat_transformed_small.toPandas()\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m climat_transformed_pandas \u001b[38;5;241m=\u001b[39m \u001b[43mclimat_transformed\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoPandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Afficher les premières lignes du DataFrame Pandas\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(climat_transformed_pandas\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/pandas/conversion.py:205\u001b[0m, in \u001b[0;36mPandasConversionMixin.toPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m rows \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(rows) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    204\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_records(\n\u001b[0;32m--> 205\u001b[0m         rows, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(rows)), columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    206\u001b[0m     )\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     pdf \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:2189\u001b[0m, in \u001b[0;36mDataFrame.columns\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2114\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m   2115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcolumns\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m   2116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2117\u001b[0m \u001b[38;5;124;03m    Retrieves the names of all columns in the :class:`DataFrame` as a list.\u001b[39;00m\n\u001b[1;32m   2118\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2187\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   2188\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [f\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[38;5;241m.\u001b[39mfields]\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:575\u001b[0m, in \u001b[0;36mDataFrame.schema\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m    572\u001b[0m             StructType, _parse_datatype_json_string(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mschema()\u001b[38;5;241m.\u001b[39mjson())\n\u001b[1;32m    573\u001b[0m         )\n\u001b[1;32m    574\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 575\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[43mPySparkValueError\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m            \u001b[49m\u001b[43merror_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCANNOT_PARSE_DATATYPE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_parameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43merror\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    578\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_schema\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/base.py:42\u001b[0m, in \u001b[0;36mPySparkException.__init__\u001b[0;34m(self, message, error_class, message_parameters)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_reader \u001b[38;5;241m=\u001b[39m ErrorClassesReader()\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43merror_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_error_message\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_class\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage_parameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m=\u001b[39m message\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/utils.py:39\u001b[0m, in \u001b[0;36mErrorClassesReader.get_error_message\u001b[0;34m(self, error_class, message_parameters)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Verify message parameters.\u001b[39;00m\n\u001b[1;32m     38\u001b[0m message_parameters_from_template \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<([a-zA-Z0-9_-]+)>\u001b[39m\u001b[38;5;124m\"\u001b[39m, message_template)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mset\u001b[39m(message_parameters_from_template) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mset\u001b[39m(message_parameters), (\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndefined error message parameter for error class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmessage_parameters\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m\u001b[38;5;241m.\u001b[39mmaketrans(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m message_template\u001b[38;5;241m.\u001b[39mtranslate(table)\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmessage_parameters)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Undefined error message parameter for error class: CANNOT_PARSE_DATATYPE. Parameters: {'error': 'An error occurred while calling o409.schema'}"
     ]
    }
   ],
   "source": [
    "# Convertir le DataFrame PySpark en DataFrame Pandas\n",
    "\n",
    "\n",
    "#climat_transformed_small = climat_transformed.limit(20)  # Exemple : limiter à 1000 lignes\n",
    "#climat_transformed_pandas = climat_transformed_small.toPandas()\n",
    "climat_transformed_pandas = climat_transformed.toPandas()\n",
    "\n",
    "# Afficher les premières lignes du DataFrame Pandas\n",
    "print(climat_transformed_pandas.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ANNEE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Extraire l'année à partir de la colonne DATE\n",
    "climat_transformed_pandas['year'] = pd.to_datetime(climat_transformed_pandas['DATE']).dt.year\n",
    "\n",
    "# Calcul de la température moyenne par année\n",
    "year_avg = climat_transformed_pandas.groupby('year')['TEMP_C'].mean().reset_index()\n",
    "\n",
    "# Ajouter les années manquantes (par exemple, de 1950 à 1955)\n",
    "all_years = pd.DataFrame({'year': range(climat_transformed_pandas['year'].min(), climat_transformed_pandas['year'].max() + 1)})\n",
    "year_avg = pd.merge(all_years, year_avg, on='year', how='left')\n",
    "year_avg['TEMP_C'] = year_avg['TEMP_C'].fillna(0)  # Remplace les valeurs manquantes\n",
    "\n",
    "# Préparer les données pour la heatmap\n",
    "heatmap_data = year_avg.set_index('year').T  # Transposer pour avoir les années comme colonnes\n",
    "\n",
    "# Dessin de la heatmap\n",
    "plt.figure(figsize=(8, 2))\n",
    "sns.heatmap(heatmap_data, cmap=\"coolwarm\", annot=True, cbar_kws={'label': 'Température moyenne (°C)'})\n",
    "plt.title(\"Températures moyennes mondiales par année\")\n",
    "plt.xlabel(\"Année\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Étape 1 : Filtrer les données\n",
    "# Exclure les jours sans précipitation (0 mm) et les valeurs aberrantes comme 99.99 mm\n",
    "precip_data_filtered = climat_transformed_pandas[(climat_transformed_pandas['PRCP'] > 0) & \n",
    "                                                 (climat_transformed_pandas['PRCP'] < 100)]['PRCP']\n",
    "\n",
    "# Vérification : S'assurer que les données filtrées ne sont pas vides\n",
    "if precip_data_filtered.empty:\n",
    "    print(\"Aucune donnée valide après le filtrage.\")\n",
    "else:\n",
    "    # Étape 2 : Créer un histogramme avec KDE pour mieux comprendre la distribution\n",
    "    plt.figure(figsize=(10, 6))  # Ajuster la taille du graphique\n",
    "    sns.histplot(precip_data_filtered, bins=30, kde=True, color=\"blue\", stat=\"density\")  # Stat=“density” pour une échelle normalisée\n",
    "\n",
    "    # Étape 3 : Ajouter des titres et des labels pour clarifier l'interprétation\n",
    "    plt.title(\"Distribution des Précipitations (mm)\", fontsize=16)  # Titre du graphique\n",
    "    plt.xlabel(\"Précipitations (mm)\", fontsize=14)  # Label de l'axe X\n",
    "    plt.ylabel(\"Densité\", fontsize=14)  # Label de l'axe Y (densité, car on utilise stat=\"density\")\n",
    "\n",
    "    # Étape 4 : Ajouter une grille pour une meilleure lisibilité\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Afficher le graphique\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
